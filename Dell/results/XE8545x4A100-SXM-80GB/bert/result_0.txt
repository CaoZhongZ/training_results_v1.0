+ : XE8545_1x4x64x1
+ : /root/localhub/language_model.sqsh
+ : 5
++ date +%y%m%d%H%M%S%N
+ : 210517234032888945621
+ : ./results
+ : ./api_logs
+ : 1
+ readonly _logfile_base=./results/210517234032888945621
+ _logfile_base=./results/210517234032888945621
+ readonly _cont_name=language_model
+ _cont_name=language_model
+ _cont_mounts=/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/2048_shards_uncompressed:/workspace/data,/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/2048_shards_uncompressed:/workspace/data_phase2,/root/mlperf_training/bert:/results,/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/cks:/workspace/phase1,/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/eval_set_uncompressed:/workspace/evaldata
+ '[' '' -eq 1 ']'
/var/spool/slurmd/job00086/slurm_script: line 46: [: : integer expression expected
+ srun --ntasks=1 --ntasks-per-node=1 mkdir -p /root/mlperf_training/bert
+ THROUGHPUT_RUN=
+ '[' -z '' ']'
+ MAX_STEPS=13700
+ PHASE1='    --train_batch_size=64     --learning_rate=3.5e-4     --warmup_proportion=0.0     --max_steps=7038     --num_steps_per_checkpoint=2500     --max_seq_length=128     --max_predictions_per_seq=20     --input_dir=/workspace/data     '
+ PHASE2='    --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt     '
+ PHASES=("$PHASE1" "$PHASE2")
+ PHASE=2
+ echo '***** Running Phase 2 *****'
***** Running Phase 2 *****
+ echo '***** SLURM_NNODES: 1 *****'
***** SLURM_NNODES: 1 *****
+ echo '***** SLURM_NTASKS: 4 *****'
***** SLURM_NTASKS: 4 *****
+ cluster=
+ [[ XE8545_1x4x64x1 == DGX2* ]]
+ [[ XE8545_1x4x64x1 == DGXA100* ]]
+ MAX_SAMPLES_TERMINATION=4500000
+ EVAL_ITER_START_SAMPLES=150000
+ EVAL_ITER_SAMPLES=150000
+ GRADIENT_STEPS=1
+ USE_DDP=0
+ BERT_CMD='    ./bind.sh --cpu=exclusive --ib=single --cluster= --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=${SLURM_LOCALID}     --bert_config_path=/workspace/phase1/bert_config.json'
+ [[ 0 != 1 ]]
+ BERT_CMD='    ./bind.sh --cpu=exclusive --ib=single --cluster= --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=${SLURM_LOCALID}     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16'
+ UNITTEST=0
+ [[ 0 != 0 ]]
+ CONTAINER_PRELOAD_LUSTRE=0
+ [[ 0 -gt 0 ]]
+ CONT_FILE=/root/localhub/language_model.sqsh
+ srun --ntasks=1 --container-image=/root/localhub/language_model.sqsh --container-name=language_model true
+ export NCCL_TOPO_FILE=/workspace/bert/dgxa100_nic_affinity.xml
+ NCCL_TOPO_FILE=/workspace/bert/dgxa100_nic_affinity.xml
++ seq 1 5
+ for _experiment_index in $(seq 1 "${NEXP}")
+ tee ./results/210517234032888945621_1.log
tee: ./results/210517234032888945621_1.log: No such file or directory
+ echo 'Beginning trial 1 of 1'
Beginning trial 1 of 1
+ '[' 1 -eq 1 ']'
+ srun --ntasks=1 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
Clearing cache on xe8545node26
vm.drop_caches = 3
+ srun --ntasks=1 --container-name=language_model python -c '
import mlperf_logger
mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)'
:::MLLOG {"namespace": "", "time_ms": 1621294835849, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ '[' 0 -eq 0 ']'
+ srun -l --mpi=none --ntasks=4 --ntasks-per-node=4 --container-name=language_model --container-mounts=/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/2048_shards_uncompressed:/workspace/data,/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/2048_shards_uncompressed:/workspace/data_phase2,/root/mlperf_training/bert:/results,/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/cks:/workspace/phase1,/xe8545_nvme0/training_datasets_v1.0/bert_wiki_hdf5/eval_set_uncompressed:/workspace/evaldata sh -c '/workspace/bert/run_and_time.sh "    ./bind.sh --cpu=exclusive --ib=single --cluster= --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1      --gradient_accumulation_steps=1     --log_freq=0     --local_rank=${SLURM_LOCALID}     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16" 414 '
3: Run vars: id 86 gpus 4 mparams 
3: STARTING TIMING RUN AT 2021-05-17 11:40:37 PM
3: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster= --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1   
3:    --gradient_accumulation_steps=1     --log_freq=0     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=414'
3: ++ ./bind.sh --cpu=exclusive --ib=single --cluster= -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --allreduce_p
3: ost_accumulation --allreduce_post_accumulation_fp16 --seed=414
2: Run vars: id 86 gpus 4 mparams 
2: STARTING TIMING RUN AT 2021-05-17 11:40:37 PM
2: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster= --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1   
2:    --gradient_accumulation_steps=1     --log_freq=0     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=414'
2: ++ ./bind.sh --cpu=exclusive --ib=single --cluster= -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --allreduce_p
2: ost_accumulation --allreduce_post_accumulation_fp16 --seed=414
0: Run vars: id 86 gpus 4 mparams 
0: STARTING TIMING RUN AT 2021-05-17 11:40:37 PM
0: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster= --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1   
0:    --gradient_accumulation_steps=1     --log_freq=0     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=414'
0: ++ ./bind.sh --cpu=exclusive --ib=single --cluster= -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --allreduce_p
0: ost_accumulation --allreduce_post_accumulation_fp16 --seed=414
1: Run vars: id 86 gpus 4 mparams 
1: STARTING TIMING RUN AT 2021-05-17 11:40:37 PM
1: + eval '     ./bind.sh --cpu=exclusive --ib=single --cluster= --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=64     --learning_rate=3.5e-4     --opt_lamb_beta_1=0.9     --opt_lamb_beta_2=0.999     --warmup_proportion=0.0     --warmup_steps=0.0     --start_warmup_step=0     --max_steps=13700     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.720     --weight_decay_rate=0.01     --max_samples_termination=4500000     --eval_iter_start_samples=150000 --eval_iter_samples=150000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding     --distributed_lamb   --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1   
1:    --gradient_accumulation_steps=1     --log_freq=0     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.json --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --seed=414'
1: ++ ./bind.sh --cpu=exclusive --ib=single --cluster= -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --allreduce_p
1: ost_accumulation --allreduce_post_accumulation_fp16 --seed=414
2: num_sockets = 2 num_nodes=8 cores_per_socket=64
2: 1 gpus_per_node
2: + exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --allreduc
2: e_post_accumulation --allreduce_post_accumulation_fp16 --seed=414
3: num_sockets = 2 num_nodes=8 cores_per_socket=64
3: 1 gpus_per_node
3: + exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --allreduc
3: e_post_accumulation --allreduce_post_accumulation_fp16 --seed=414
0: num_sockets = 2 num_nodes=8 cores_per_socket=64
0: 1 gpus_per_node
0: + exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --allreduce
0: _post_accumulation --allreduce_post_accumulation_fp16 --seed=414
1: num_sockets = 2 num_nodes=8 cores_per_socket=64
1: 1 gpus_per_node
1: + exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=64 --learning_rate=3.5e-4 --opt_lamb_beta_1=0.9 --opt_lamb_beta_2=0.999 --warmup_proportion=0.0 --warmup_steps=0.0 --start_warmup_step=0 --max_steps=13700 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.720 --weight_decay_rate=0.01 --max_samples_termination=4500000 --eval_iter_start_samples=150000 --eval_iter_samples=150000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --fused_mha --dense_seq_output --unpad --unpad_fmha --exchange_padding --distributed_lamb --dwu-num-rs-pg=1 --dwu-num-ar-pg=1 --dwu-num-blocks=1 --gradient_accumulation_steps=1 --log_freq=0 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --allreduc
1: e_post_accumulation --allreduce_post_accumulation_fp16 --seed=414
0: :::MLLOG {"namespace": "", "time_ms": 1621294839123, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
2: :::MLLOG {"namespace": "", "time_ms": 1621294839137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
3: :::MLLOG {"namespace": "", "time_ms": 1621294839149, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
1: :::MLLOG {"namespace": "", "time_ms": 1621294839154, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 996}}
0: device: cuda:0 n_gpu: 4, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1621294839199, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 67}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839199, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "Dell", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 72}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839199, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 76}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839199, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 80}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839199, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "1xXE8545x4A100-SXM-80GB", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 84}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839199, "event_type": "POINT_IN_TIME", "key": "seed", "value": 414, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1006}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839200, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 256, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1008}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839200, "event_type": "POINT_IN_TIME", "key": "d_batch_size", "value": 64, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1010}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839200, "event_type": "POINT_IN_TIME", "key": "gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1012}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839200, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1014}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839200, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 13700.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1016}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294839200, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1018}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_dropout_add=False, fused_gelu_bias=True, fused_mha=True, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, input_d
0: ir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=13700.0, min_samples_to_start_checkpoints=3000000, n_gpu=4, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, phase2=True, resume_from_checkpoint=False, seed=414, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=64, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
2: device: cuda:2 n_gpu: 4, distributed training: True, 16-bits training: True
3: device: cuda:3 n_gpu: 4, distributed training: True, 16-bits training: True
1: device: cuda:1 n_gpu: 4, distributed training: True, 16-bits training: True
0: :::MLLOG {"namespace": "", "time_ms": 1621294846159, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.00035, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 669}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294846206, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 699}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294846206, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.9, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 702}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294846206, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.999, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 703}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294846206, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 704}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294846215, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294846215, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 87}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294846215, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 88}}
1: Torch distributed is available.
1: Torch distributed is initialized.
0: Torch distributed is available.
0: Torch distributed is initialized.
3: Torch distributed is available.
3: Torch distributed is initialized.
2: Torch distributed is available.
2: Torch distributed is initialized.
0: :::MLLOG {"namespace": "", "time_ms": 1621294854720, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1264}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294854725, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1265}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294854739, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1276, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1621294854740, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1278, "first_epoch_num": 1, "epoch_count": 1}}
0: parsed args:
0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', bypass_amp=False, cache_eval_data=True, checkpoint_activations=False, cuda_graph_mode='segmented', ddp_type='apex', dense_seq_output=True, device=device(type='cuda', index=0), disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, distributed_lamb=True, do_train=True, dwu_e5m2_allgather=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=1, dwu_num_blocks=1, dwu_num_chunks=1, dwu_num_rs_pg=1, dwu_overlap_reductions=False, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=150000, eval_iter_start_samples=150000, exchange_padding=True, fp16=True, fused_dropout_add=False, fused_gelu_bias=True, fused_mha=True, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, input_d
0: ir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.00035, local_rank=0, log_freq=0.0, loss_scale=0.0, max_iterations_per_graph=4, max_predictions_per_seq=76, max_samples_termination=4500000.0, max_seq_length=512, max_steps=13700.0, min_samples_to_start_checkpoints=3000000, n_gpu=4, num_epochs_to_generate_seeds_for=2, num_eval_examples=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.9, opt_lamb_beta_2=0.999, output_dir='/results', pad=False, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=414, skip_checkpoint=True, start_warmup_step=0.0, target_mlm_accuracy=0.72, train_batch_size=64, train_mlm_accuracy_window_size=0, unpad=True, unpad_fmha=True, use_cuda_graph=False, use_ddp=False, use_env=False, use_gradient_as_bucket_view=False, warmup_proportion=0.0, warmup_steps=0.0, weight_decay_rate=0.01)
0: epoch: 1
0: :::MLLOG {"namespace": "", "time_ms": 1621294978266, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.37728947401046753, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 586, 'eval_loss': 4.074650287628174, 'eval_mlm_accuracy': 0.37728947401046753}
0: :::MLLOG {"namespace": "", "time_ms": 1621295099206, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4196995496749878, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 1172, 'eval_loss': 3.691861867904663, 'eval_mlm_accuracy': 0.4196995496749878}
0: :::MLLOG {"namespace": "", "time_ms": 1621295227940, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.4901987612247467, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 1758, 'eval_loss': 3.076301336288452, 'eval_mlm_accuracy': 0.4901987612247467}
0: :::MLLOG {"namespace": "", "time_ms": 1621295356284, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6641149520874023, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 2344, 'eval_loss': 1.6876766681671143, 'eval_mlm_accuracy': 0.6641149520874023}
0: :::MLLOG {"namespace": "", "time_ms": 1621295479134, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.701842725276947, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 2930, 'eval_loss': 1.4225643873214722, 'eval_mlm_accuracy': 0.701842725276947}
0: :::MLLOG {"namespace": "", "time_ms": 1621295600905, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7060419321060181, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 3516, 'eval_loss': 1.3920692205429077, 'eval_mlm_accuracy': 0.7060419321060181}
0: :::MLLOG {"namespace": "", "time_ms": 1621295731799, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7093632221221924, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 4102, 'eval_loss': 1.3719767332077026, 'eval_mlm_accuracy': 0.7093632221221924}
0: :::MLLOG {"namespace": "", "time_ms": 1621295863534, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7105337977409363, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 4688, 'eval_loss': 1.3625919818878174, 'eval_mlm_accuracy': 0.7105337977409363}
0: :::MLLOG {"namespace": "", "time_ms": 1621295989782, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7121897339820862, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 5274, 'eval_loss': 1.3534868955612183, 'eval_mlm_accuracy': 0.7121897339820862}
0: :::MLLOG {"namespace": "", "time_ms": 1621296114975, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7124243378639221, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 5860, 'eval_loss': 1.353166937828064, 'eval_mlm_accuracy': 0.7124243378639221}
0: :::MLLOG {"namespace": "", "time_ms": 1621296269840, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7148165702819824, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 6446, 'eval_loss': 1.3393776416778564, 'eval_mlm_accuracy': 0.7148165702819824}
0: :::MLLOG {"namespace": "", "time_ms": 1621296431446, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.715355396270752, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 7032, 'eval_loss': 1.3362720012664795, 'eval_mlm_accuracy': 0.715355396270752}
0: :::MLLOG {"namespace": "", "time_ms": 1621296571034, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7159871459007263, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 7618, 'eval_loss': 1.330888032913208, 'eval_mlm_accuracy': 0.7159871459007263}
0: :::MLLOG {"namespace": "", "time_ms": 1621296703566, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.716651439666748, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 8204, 'eval_loss': 1.3248872756958008, 'eval_mlm_accuracy': 0.716651439666748}
0: :::MLLOG {"namespace": "", "time_ms": 1621296843229, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7174085974693298, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 8790, 'eval_loss': 1.3217095136642456, 'eval_mlm_accuracy': 0.7174085974693298}
0: :::MLLOG {"namespace": "", "time_ms": 1621296984602, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7179427742958069, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 9375, 'eval_loss': 1.3193082809448242, 'eval_mlm_accuracy': 0.7179427742958069}
0: :::MLLOG {"namespace": "", "time_ms": 1621297120082, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7182586193084717, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 9961, 'eval_loss': 1.3133450746536255, 'eval_mlm_accuracy': 0.7182586193084717}
0: :::MLLOG {"namespace": "", "time_ms": 1621297251765, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7191597819328308, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 10547, 'eval_loss': 1.3073891401290894, 'eval_mlm_accuracy': 0.7191597819328308}
0: :::MLLOG {"namespace": "", "time_ms": 1621297389153, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7197752594947815, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 11133, 'eval_loss': 1.305747389793396, 'eval_mlm_accuracy': 0.7197752594947815}
0: :::MLLOG {"namespace": "", "time_ms": 1621297528965, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7201654314994812, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1441, "epoch_num": 1}}
0: {'global_steps': 11719, 'eval_loss': 1.3009741306304932, 'eval_mlm_accuracy': 0.7201654314994812}
0: 0.720165 > 0.720000, Target MLM Accuracy reached at 11719
0: (1, 11731.0) {'final_loss': 0.0}
0: :::MLLOG {"namespace": "", "time_ms": 1621297529008, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1567, "first_epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1621297529008, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1570, "epoch_num": 1}}
0: :::MLLOG {"namespace": "", "time_ms": 1621297529008, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3000064, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1574}}
0: :::MLLOG {"namespace": "", "time_ms": 1621297529009, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1577}}
0: :::MLLOG {"namespace": "", "time_ms": 1621297529009, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 1580, "status": "success"}}
0: {'e2e_time': 2689.9456849098206, 'training_sequences_per_second': 1308.0424410718313, 'final_loss': 0.0, 'raw_train_time': 2681.2585661411285}
0: ++ date +%s
0: + END=1621297529
0: ++ date '+%Y-%m-%d %r'
0: + END_FMT='2021-05-18 12:25:29 AM'
0: ENDING TIMING RUN AT 2021-05-18 12:25:29 AM
0: + echo 'ENDING TIMING RUN AT 2021-05-18 12:25:29 AM'
0: + RESULT=2692
0: + RESULT_NAME=bert
0: RESULT,bert,414,2692,root,2021-05-17 11:40:37 PM
0: + echo 'RESULT,bert,414,2692,root,2021-05-17 11:40:37 PM'
0: + set +x
2: ++ date +%s
2: + END=1621297530
2: ++ date '+%Y-%m-%d %r'
2: + END_FMT='2021-05-18 12:25:30 AM'
2: ENDING TIMING RUN AT 2021-05-18 12:25:30 AM
2: + echo 'ENDING TIMING RUN AT 2021-05-18 12:25:30 AM'
2: + RESULT=2693
2: + RESULT_NAME=bert
2: RESULT,bert,414,2693,root,2021-05-17 11:40:37 PM
2: + echo 'RESULT,bert,414,2693,root,2021-05-17 11:40:37 PM'
2: + set +x
3: ++ date +%s
3: + END=1621297530
3: ++ date '+%Y-%m-%d %r'
3: + END_FMT='2021-05-18 12:25:30 AM'
3: + echo 'ENDING TIMING RUN AT 2021-05-18 12:25:30 AM'
3: ENDING TIMING RUN AT 2021-05-18 12:25:30 AM
3: + RESULT=2693
3: + RESULT_NAME=bert
3: RESULT,bert,414,2693,root,2021-05-17 11:40:37 PM
3: + echo 'RESULT,bert,414,2693,root,2021-05-17 11:40:37 PM'
3: + set +x
1: ++ date +%s
1: + END=1621297530
1: ++ date '+%Y-%m-%d %r'
1: + END_FMT='2021-05-18 12:25:30 AM'
1: + echo 'ENDING TIMING RUN AT 2021-05-18 12:25:30 AM'
1: ENDING TIMING RUN AT 2021-05-18 12:25:30 AM
1: + RESULT=2693
1: + RESULT_NAME=bert
1: RESULT,bert,414,2693,root,2021-05-17 11:40:37 PM
1: + echo 'RESULT,bert,414,2693,root,2021-05-17 11:40:37 PM'
1: + set +x
