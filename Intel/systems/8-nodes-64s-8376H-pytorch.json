{
    "division": "open",
    "submitter": "Intel",
    "status": "available",
    "system_name": "8-nodes-64s-8376H-pytorch",
    "number_of_nodes": "8",
    "host_processor_model_name": "Intel(R) Xeon(R) Platinum 8376H CPU @ 2.60GHz",
    "host_processors_per_node": "8",
    "host_processor_core_count": "28",
    "host_processor_frequency": "2.6GHz",
    "host_processor_caches": "",
    "host_memory_configuration": "6 slots / 32GB each / 3200 MT/s per socket",     
    "host_memory_capacity": "1536GB",
    "host_storage_capacity": "",
    "host_storage_type": "",
    "host_processor_interconnect": "",
    "host_networking": "",
    "host_networking_topology": "",
    "host_processor_vcpu_count": "448", 

    "accelerators_per_node": "0",
    "accelerator_model_name": "N/A",
    "accelerator_frequency": "",
    "accelerator_host_interconnect": "",
    "accelerator_interconnect": "",
    "accelerator_interconnect_topology": "",
    "accelerator_memory_capacity": "N/A",
    "accelerator_memory_configuration": "",
    "accelerator_on-chip_memories": "",
    "cooling": "",
    "hw_notes": "",

    "framework": "PyTorch v1.5",
    "operating_system": "Red Hat Enterprise Linux 8.2 (Ootpa)",
    "other_software_stack": "IPEX v0.2, Torch-CCL v1.1.0, oneCCL 2021.2.1",
    "sw_notes": "We use hybrid optimizer, LAMB for data-parallel layers and SGD for embedding, for large-batch training"
}
