# BERT

## Benchmark Information

BERT is the
[BERT masked language modeling](https://github.com/mlperf/training/tree/master/language_model/tensorflow/bert) benchmark.

## Software

Tensorflow v1.

## Hardware

TPU v4.

## Model
### Publication/Attribution

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. [BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding]
(https://arxiv.org/abs/1810.04805). NAACL 2019.

## Dataset Preparation

*   [BERT Wikipedia dataset preparation](https://github.com/mlperf/training/tree/master/language_model/tensorflow/bert#download-and-preprocess-datasets)

## Research submissions

Preview submissions were run using Google internal infrastructure.
Contact Peter Mattson (petermattson@google.com) for more details.
